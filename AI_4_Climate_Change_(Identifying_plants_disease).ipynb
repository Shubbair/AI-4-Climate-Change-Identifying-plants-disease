{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Kq9FjgRAbX0",
        "2cl-bQKQAj_c",
        "J7a9QZmUAQy6",
        "crWMqC7ZwnRv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarrajaa/AI-4-Climate-Change-Identifying-plants-disease/blob/main/AI_4_Climate_Change_(Identifying_plants_disease).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying plants disease project "
      ],
      "metadata": {
        "id": "lQLhMaVap5u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we need to import libraries  and put random state"
      ],
      "metadata": {
        "id": "ivdRdLNWrJw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#libraties\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "import os\n",
        "#random state\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "PYump6gDryj4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Download , Arrangement  and Read data**"
      ],
      "metadata": {
        "id": "FCeIPIi1_i-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 download"
      ],
      "metadata": {
        "id": "_Kq9FjgRAbX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "lWa9yjsXxDpH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d smaranjitghose/corn-or-maize-leaf-disease-dataset\n",
        "! unzip corn-or-maize-leaf-disease-dataset"
      ],
      "metadata": {
        "id": "ltf8DMEJAGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 arrangment"
      ],
      "metadata": {
        "id": "2cl-bQKQAj_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('PlantsData')\n",
        "os.mkdir('PlantsData/corn')"
      ],
      "metadata": {
        "id": "OwfIvMESVPZ0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'data'\n",
        "destination = 'corn'\n",
        "# gather all files\n",
        "allfiles = os.listdir(source)\n",
        " \n",
        "# iterate on all files to move them to destination folder\n",
        "for f in allfiles:\n",
        "    src_path = os.path.join(source, f)\n",
        "    dst_path = os.path.join(destination, f)\n",
        "    os.rename(src_path, dst_path)"
      ],
      "metadata": {
        "id": "PG49hdYdJqGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 read\n"
      ],
      "metadata": {
        "id": "J7a9QZmUAQy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(path):  # e.g. path = cats_and_dogs_filtered/train\n",
        "    x = []  # images\n",
        "    y = []  # labels\n",
        "\n",
        "    # we will use os.listdir to read the contents of the folder\n",
        "    labels = os.listdir(path)  # e.g. labels = [\"cats\", \"dogs\"]\n",
        "\n",
        "    # iterate over the labels\n",
        "    for label in labels:\n",
        "        # we join the label to the path to get the path to the class folder which contains the images\n",
        "        label_dir = os.path.join(path, label)  # e.g. cats_and_dogs_filtered/train/cat\n",
        "\n",
        "        # we will use os.listdir again to read the contents of the class folders (i.e., the images)\n",
        "        images = os.listdir(label_dir)  # e.g. [\"cat.100.jpg\", \"cat.101.jpg\", ...]\n",
        "\n",
        "        # iterate over the images\n",
        "        for image in os.listdir(label_dir):\n",
        "            # we join the image name to the path to get the path to the image\n",
        "            image_path = os.path.join(\n",
        "                label_dir, image\n",
        "            )  # e.g. cats_and_dogs_filtered/train/cats/cat.100.jpg\n",
        "\n",
        "            # append the image path and the label to the lists\n",
        "            x.append(image_path)\n",
        "            y.append(label)\n",
        "\n",
        "    # return a dataframe with the image paths and labels\n",
        "    return pd.DataFrame({\"image\": x, \"label\": y})\n",
        "\n",
        "\n",
        "# read the training and validation sets\n",
        "train_df = read_dataset(\"data\")"
      ],
      "metadata": {
        "id": "KgAqycNm78zV"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. processing data (pipelines)**"
      ],
      "metadata": {
        "id": "crWMqC7ZwnRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_index = {\n",
        "    'Blight': 0,\n",
        "    'Common_Rust': 1,\n",
        "    'Gray_Leaf_Spot': 2,\n",
        "    'Healthy': 3,\n",
        "}\n",
        "\n",
        "def image_to_resnet_tensor(x, y):\n",
        "    # read the image from the path\n",
        "    img = tf.io.read_file(x)\n",
        "    # decode the image\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # resize the image\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    # normalize the image\n",
        "    img = img/225.0\n",
        "    return img, y\n",
        "\n",
        "def label_to_one_hot(x, y):\n",
        "  y = tf.one_hot(y, depth=4)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def image_augmentation(x, y):\n",
        "    # randomly flip the image horizontally\n",
        "    img = tf.image.random_flip_left_right(x)\n",
        "    # randomly flip the image vertically\n",
        "    img = tf.image.random_flip_up_down(img)\n",
        "    # randomly change the brightness of the image\n",
        "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
        "    # clip the image to be between 0 and 1\n",
        "    img = tf.clip_by_value(img, 0, 1)\n",
        "    return img, y\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label_to_index)"
      ],
      "metadata": {
        "id": "yUsua1-thhCb"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset_creator(x, y, training=False):\n",
        "    # create a tf.data.Dataset from the input output pairs\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_df[\"image\"], train_df[\"label\"]))\n",
        "    # map the image paths to tensors using the new resnet preprocessing function\n",
        "    dataset = dataset.map(image_to_resnet_tensor)\n",
        "    # map the labels to one-hot encoded vectors\n",
        "    dataset = dataset.map(label_to_one_hot)\n",
        "    # if training, apply image augmentation. Remember that we never apply image augmentation to the validation set\n",
        "    if training:\n",
        "        dataset = dataset.map(image_augmentation)\n",
        "    # shuffle the dataset\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    # batch the dataset\n",
        "    dataset = dataset.batch(32)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = tf_dataset_creator(train_df[\"image\"], train_df[\"label\"], training=True)"
      ],
      "metadata": {
        "id": "tgnmAMdswzId"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_label = {\n",
        "    0: 'Bilght',\n",
        "    1: 'Common_Rust',\n",
        "    2: 'Gray_Leaf_Spot',\n",
        "    3: 'Healthy',\n",
        "}\n",
        "\n",
        "for image, label in train_dataset.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(20):\n",
        "        ax = plt.subplot(5, 5, i + 1)\n",
        "        plt.imshow(image[i].numpy())\n",
        "        plt.title(index_to_label[np.argmax(label[i].numpy())])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "9lEm2q259rpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}